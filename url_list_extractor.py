#!/usr/bin/env python3
"""
URLãƒªã‚¹ãƒˆæŠ½å‡ºãƒ„ãƒ¼ãƒ«

ã‚·ãƒ¼ãƒˆãƒªã‚¹ãƒˆã‹ã‚‰å°æœ¬URLã‚’æŠ½å‡ºã—ã€
å®Ÿéš›ã®å°æœ¬ã®æ§‹é€ ã¨æŠ½å‡ºçŠ¶æ³ã‚’ç¢ºèªã™ã‚‹
"""

import sqlite3
import requests
import pandas as pd
import io
import re
from datetime import datetime

class UrlListExtractor:
    def __init__(self, db_path):
        self.db_path = db_path
        self.log_file = "/Users/mitsuruono/sunsun_script_search/sunsun_script_database/url_list_extraction.txt"
        
        # URLãƒªã‚¹ãƒˆã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ
        self.url_list_sheet = 'https://docs.google.com/spreadsheets/d/1c_txRaInj7yQUFBZLBSj65pLzhKxHofsobLJyM065g8/edit?gid=1092002230#gid=1092002230'
    
    def log_message(self, message: str):
        """ãƒ­ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‡ºåŠ›"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_entry = f"[{timestamp}] {message}\n"
        
        with open(self.log_file, "a", encoding="utf-8") as f:
            f.write(log_entry)
        
        print(log_entry.strip())
    
    def extract_urls_from_list(self):
        """URLãƒªã‚¹ãƒˆã‹ã‚‰å°æœ¬URLã‚’æŠ½å‡º"""
        try:
            # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆIDã€GIDæŠ½å‡º
            sheet_pattern = r'/spreadsheets/d/([a-zA-Z0-9-_]+)'
            gid_pattern = r'[#&]gid=([0-9]+)'
            
            sheet_match = re.search(sheet_pattern, self.url_list_sheet)
            gid_match = re.search(gid_pattern, self.url_list_sheet)
            
            if not sheet_match:
                return []
            
            spreadsheet_id = sheet_match.group(1)
            gid = gid_match.group(1) if gid_match else '0'
            
            # CSVå‡ºåŠ›URL
            csv_url = f"https://docs.google.com/spreadsheets/d/{spreadsheet_id}/export?format=csv&gid={gid}"
            
            # CSVãƒ‡ãƒ¼ã‚¿å–å¾—
            response = requests.get(csv_url, timeout=15)
            if response.status_code != 200:
                return []
            
            # CSVè§£æ
            csv_data = response.content.decode('utf-8', errors='ignore')
            df = pd.read_csv(io.StringIO(csv_data))
            
            self.log_message(f"ğŸ“‹ URLãƒªã‚¹ãƒˆ: {len(df)}è¡Œ x {len(df.columns)}åˆ—")
            
            # å°æœ¬URLåˆ—ã‚’ç‰¹å®šï¼ˆé€šå¸¸ã¯5åˆ—ç›®ï¼‰
            script_urls = []
            
            for row_idx in range(len(df)):
                row = df.iloc[row_idx]
                
                # ç®¡ç†ç•ªå·ï¼ˆé€šå¸¸ã¯3åˆ—ç›®ï¼‰
                management_id = ""
                if len(row) > 3 and pd.notna(row.iloc[3]):
                    management_id = str(row.iloc[3]).strip()
                
                # ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆé€šå¸¸ã¯4åˆ—ç›®ï¼‰
                title = ""
                if len(row) > 4 and pd.notna(row.iloc[4]):
                    title = str(row.iloc[4]).strip()
                
                # å°æœ¬URLï¼ˆé€šå¸¸ã¯5åˆ—ç›®ï¼‰
                script_url = ""
                if len(row) > 5 and pd.notna(row.iloc[5]):
                    url_value = str(row.iloc[5]).strip()
                    if 'docs.google.com' in url_value:
                        script_url = url_value
                
                if management_id and script_url:
                    script_urls.append({
                        'management_id': management_id,
                        'title': title,
                        'script_url': script_url
                    })
            
            self.log_message(f"ğŸ“Š æŠ½å‡ºã•ã‚ŒãŸå°æœ¬URL: {len(script_urls)}ä»¶")
            return script_urls
            
        except Exception as e:
            self.log_message(f"âŒ URLãƒªã‚¹ãƒˆæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []
    
    def analyze_sample_scripts(self, script_urls, sample_count=10):
        """ã‚µãƒ³ãƒ—ãƒ«å°æœ¬ã®è©³ç´°åˆ†æ"""
        self.log_message(f"\nğŸ“– ã‚µãƒ³ãƒ—ãƒ«å°æœ¬åˆ†æï¼ˆ{sample_count}ä»¶ï¼‰:")
        
        for i, script_info in enumerate(script_urls[:sample_count]):
            self.log_message(f"\n{'=' * 50}")
            self.log_message(f"ã‚µãƒ³ãƒ—ãƒ« {i+1}: {script_info['management_id']}")
            self.log_message(f"ã‚¿ã‚¤ãƒˆãƒ«: {script_info['title'][:50]}...")
            self.log_message(f"{'=' * 50}")
            
            # å°æœ¬æ§‹é€ åˆ†æ
            self.analyze_single_script(script_info)
    
    def analyze_single_script(self, script_info):
        """å˜ä¸€å°æœ¬ã®æ§‹é€ åˆ†æ"""
        try:
            # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆIDã€GIDæŠ½å‡º
            sheet_pattern = r'/spreadsheets/d/([a-zA-Z0-9-_]+)'
            gid_pattern = r'[#&]gid=([0-9]+)'
            
            sheet_match = re.search(sheet_pattern, script_info['script_url'])
            gid_match = re.search(gid_pattern, script_info['script_url'])
            
            if not sheet_match:
                self.log_message(f"âŒ {script_info['management_id']}: ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆIDæŠ½å‡ºå¤±æ•—")
                return
            
            spreadsheet_id = sheet_match.group(1)
            gid = gid_match.group(1) if gid_match else '0'
            
            # CSVå‡ºåŠ›URL
            csv_url = f"https://docs.google.com/spreadsheets/d/{spreadsheet_id}/export?format=csv&gid={gid}"
            
            # CSVãƒ‡ãƒ¼ã‚¿å–å¾—
            response = requests.get(csv_url, timeout=10)
            if response.status_code != 200:
                self.log_message(f"âŒ {script_info['management_id']}: HTTP {response.status_code}")
                return
            
            # CSVè§£æ
            csv_data = response.content.decode('utf-8', errors='ignore')
            df = pd.read_csv(io.StringIO(csv_data))
            
            self.log_message(f"ğŸ“Š {script_info['management_id']}: {len(df)}è¡Œ x {len(df.columns)}åˆ—")
            
            # æœ€åˆã®10è¡Œã‚’è¡¨ç¤º
            self.log_message(f"ğŸ” æœ€åˆã®10è¡Œ:")
            for row_idx in range(min(10, len(df))):
                row = df.iloc[row_idx]
                row_data = []
                for col_idx, value in enumerate(row):
                    if pd.notna(value):
                        value_str = str(value).strip()
                        if len(value_str) > 0:
                            if len(value_str) > 25:
                                value_str = value_str[:25] + "..."
                            row_data.append(f"åˆ—{col_idx}:'{value_str}'")
                
                if row_data:
                    self.log_message(f"  è¡Œ{row_idx}: {' | '.join(row_data[:4])}")
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼æ¤œç´¢
            character_headers = []
            dialogue_headers = []
            
            for row_idx in range(min(15, len(df))):
                row = df.iloc[row_idx]
                for col_idx, value in enumerate(row):
                    if pd.notna(value):
                        value_str = str(value).strip().lower()
                        
                        # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ãƒ˜ãƒƒãƒ€ãƒ¼
                        if any(keyword in value_str for keyword in ['ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼', 'ã‚­ãƒ£ãƒ©', 'character']):
                            character_headers.append(f"è¡Œ{row_idx}åˆ—{col_idx}")
                        
                        # ã‚»ãƒªãƒ•ãƒ˜ãƒƒãƒ€ãƒ¼
                        if any(keyword in value_str for keyword in ['ã‚»ãƒªãƒ•', 'ã›ã‚Šãµ', 'dialogue']):
                            dialogue_headers.append(f"è¡Œ{row_idx}åˆ—{col_idx}")
            
            self.log_message(f"ğŸ“ ãƒ˜ãƒƒãƒ€ãƒ¼æƒ…å ±:")
            self.log_message(f"  ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ãƒ˜ãƒƒãƒ€ãƒ¼: {character_headers if character_headers else 'è¦‹ã¤ã‹ã‚‰ãš'}")
            self.log_message(f"  ã‚»ãƒªãƒ•ãƒ˜ãƒƒãƒ€ãƒ¼: {dialogue_headers if dialogue_headers else 'è¦‹ã¤ã‹ã‚‰ãš'}")
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã§ã®æŠ½å‡ºçŠ¶æ³ç¢ºèª
            self.check_database_status(script_info['management_id'])
            
        except Exception as e:
            self.log_message(f"âŒ {script_info['management_id']}: åˆ†æã‚¨ãƒ©ãƒ¼ - {str(e)}")
    
    def check_database_status(self, management_id):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã§ã®æŠ½å‡ºçŠ¶æ³ç¢ºèª"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT COUNT(cdu.id) as char_count
                FROM scripts s
                LEFT JOIN character_dialogue_unified cdu ON s.id = cdu.script_id
                WHERE s.management_id = ?
                GROUP BY s.id
            """, (management_id,))
            
            result = cursor.fetchone()
            conn.close()
            
            if result:
                char_count = result[0]
                self.log_message(f"ğŸ—„ï¸  ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹: {char_count}ä»¶ã®ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿")
                if char_count < 5:
                    self.log_message(f"  âš ï¸  ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªãã€æŠ½å‡ºä¸å‚™ã®å¯èƒ½æ€§")
                else:
                    self.log_message(f"  âœ… é©åˆ‡ã«ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ãŒæŠ½å‡ºã•ã‚Œã¦ã„ã¾ã™")
            else:
                self.log_message(f"ğŸ—„ï¸  ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹: ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            
        except Exception as e:
            self.log_message(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç¢ºèªã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    def run_comprehensive_analysis(self):
        """åŒ…æ‹¬çš„åˆ†æå®Ÿè¡Œ"""
        self.log_message("=" * 80)
        self.log_message("URLãƒªã‚¹ãƒˆå°æœ¬åˆ†æé–‹å§‹")
        self.log_message("=" * 80)
        
        # URLãƒªã‚¹ãƒˆã‹ã‚‰å°æœ¬URLæŠ½å‡º
        script_urls = self.extract_urls_from_list()
        
        if not script_urls:
            self.log_message("âŒ å°æœ¬URLãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return
        
        # ã‚µãƒ³ãƒ—ãƒ«åˆ†æ
        self.analyze_sample_scripts(script_urls, 10)
        
        # 2019å¹´ãƒ‡ãƒ¼ã‚¿ã®ç‰¹åˆ¥ç¢ºèª
        self.log_message(f"\n{'=' * 80}")
        self.log_message("2019å¹´ãƒ‡ãƒ¼ã‚¿ç‰¹åˆ¥ç¢ºèª")
        self.log_message(f"{'=' * 80}")
        
        year_2019_scripts = [s for s in script_urls if '19/' in s.get('title', '') or 'B5' in s.get('management_id', '') or 'B6' in s.get('management_id', '')]
        
        if year_2019_scripts:
            self.log_message(f"ğŸ—“ï¸ 2019å¹´é–¢é€£ã‚¹ã‚¯ãƒªãƒ—ãƒˆ: {len(year_2019_scripts)}ä»¶ç™ºè¦‹")
            self.analyze_sample_scripts(year_2019_scripts[:5], 5)
        else:
            self.log_message("ğŸ—“ï¸ 2019å¹´é–¢é€£ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        
        self.log_message("=" * 80)
        self.log_message("åˆ†æå®Œäº†")
        self.log_message("=" * 80)

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    db_path = "/Users/mitsuruono/sunsun_script_search/sunsun_script_database/youtube_search_complete_all.db"
    
    extractor = UrlListExtractor(db_path)
    
    print("=== URLãƒªã‚¹ãƒˆå°æœ¬åˆ†æãƒ„ãƒ¼ãƒ« ===")
    
    # åŒ…æ‹¬åˆ†æå®Ÿè¡Œ
    extractor.run_comprehensive_analysis()
    
    print(f"\nâœ… åˆ†æå®Œäº†ï¼")
    print(f"è©³ç´°ã¯ url_list_extraction.txt ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

if __name__ == "__main__":
    main()