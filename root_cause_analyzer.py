#!/usr/bin/env python3
"""
Root Cause Analyzer

This script analyzes why character-dialogue adjacency logic fails
and causes instruction text to be misclassified as dialogue.
"""

import sqlite3
import requests
import pandas as pd
import io
import re
from datetime import datetime

class RootCauseAnalyzer:
    def __init__(self, db_path):
        self.db_path = db_path
        self.log_file = "/Users/mitsuruono/sunsun_script_search/sunsun_script_database/root_cause_analysis_log.txt"
        
    def log_message(self, message: str):
        """Log messages with timestamp"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_entry = f"[{timestamp}] {message}\n"
        
        with open(self.log_file, "a", encoding="utf-8") as f:
            f.write(log_entry)
        
        print(log_entry.strip())
    
    def analyze_problematic_entries(self, limit=20):
        """Analyze specific problematic entries to understand the cause"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Get problematic entries with clear instruction markers
            cursor.execute('''
                SELECT cdu.character_name, cdu.dialogue_text, cdu.filming_audio_instructions, 
                       cdu.row_number, s.management_id, s.title, s.script_url
                FROM character_dialogue_unified cdu
                JOIN scripts s ON cdu.script_id = s.id
                WHERE LENGTH(cdu.character_name) > 0 AND LENGTH(cdu.dialogue_text) > 0
                AND (cdu.dialogue_text LIKE '%ãƒ»%' OR cdu.dialogue_text LIKE '%ï¼ˆ%' 
                     OR cdu.dialogue_text LIKE '%ä½œã‚Šæ–¹%' OR cdu.dialogue_text LIKE '%æ¼”å‡º%'
                     OR cdu.dialogue_text LIKE '%ã‚·ãƒ¼ãƒ³%' OR cdu.dialogue_text LIKE '%ã‚¢ãƒ‹ãƒ¡%')
                ORDER BY LENGTH(cdu.dialogue_text) DESC
                LIMIT ?
            ''', (limit,))
            
            problematic_entries = cursor.fetchall()
            conn.close()
            
            self.log_message("=" * 80)
            self.log_message("å•é¡Œã‚¨ãƒ³ãƒˆãƒªã®è©³ç´°åˆ†æ")
            self.log_message("=" * 80)
            
            for i, (char_name, dialogue, instructions, row_num, script_id, title, script_url) in enumerate(problematic_entries, 1):
                self.log_message(f"\n{i}. ã‚¹ã‚¯ãƒªãƒ—ãƒˆ: {script_id} - è¡Œ{row_num}")
                self.log_message(f"   ã‚¿ã‚¤ãƒˆãƒ«: {title[:50]}...")
                self.log_message(f"   ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼: {char_name}")
                self.log_message(f"   ã‚»ãƒªãƒ•: {dialogue[:200]}...")
                self.log_message(f"   æŒ‡ç¤º: {instructions[:100]}...")
                
                # Analyze the original spreadsheet structure for this entry
                if script_url and i <= 5:  # Only analyze first 5 to avoid too many requests
                    self.analyze_original_structure(script_url, script_id, row_num, char_name, dialogue)
            
            return problematic_entries
            
        except Exception as e:
            self.log_message(f"âŒ Analysis error: {str(e)}")
            return []
    
    def analyze_original_structure(self, script_url, script_id, row_num, char_name, dialogue):
        """Analyze the original spreadsheet structure to understand extraction logic"""
        if not script_url or 'docs.google.com' not in script_url:
            return
        
        try:
            # Extract spreadsheet ID and GID
            sheet_pattern = r'/spreadsheets/d/([a-zA-Z0-9-_]+)'
            gid_pattern = r'[#&]gid=([0-9]+)'
            
            sheet_match = re.search(sheet_pattern, script_url)
            gid_match = re.search(gid_pattern, script_url)
            
            if not sheet_match:
                return
            
            spreadsheet_id = sheet_match.group(1)
            gid = gid_match.group(1) if gid_match else '0'
            
            # Build CSV export URL
            csv_url = f"https://docs.google.com/spreadsheets/d/{spreadsheet_id}/export?format=csv&gid={gid}"
            
            # Fetch CSV data
            response = requests.get(csv_url, timeout=10)
            if response.status_code != 200:
                self.log_message(f"   âš ï¸  HTTP {response.status_code} for {script_id}")
                return
            
            # Parse CSV
            csv_data = response.content.decode('utf-8', errors='ignore')
            df = pd.read_csv(io.StringIO(csv_data))
            
            self.log_message(f"\\n   ğŸ“Š {script_id} å…ƒã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆåˆ†æ:")
            self.log_message(f"   ç·è¡Œæ•°: {len(df)}")
            self.log_message(f"   ç·åˆ—æ•°: {len(df.columns)}")
            
            # Show column headers
            self.log_message(f"   åˆ—å: {list(df.columns)[:10]}")
            
            # Analyze the specific row
            if row_num < len(df):
                target_row = df.iloc[row_num]
                self.log_message(f"\\n   è¡Œ{row_num}ã®å…¨ãƒ‡ãƒ¼ã‚¿:")
                
                for col_idx, (col_name, value) in enumerate(target_row.items()):
                    if pd.notna(value) and str(value).strip():
                        value_str = str(value).strip()
                        self.log_message(f"     åˆ—{col_idx}({col_name}): {value_str[:100]}...")
                        
                        # Check if this matches our extracted character or dialogue
                        if char_name in value_str:
                            self.log_message(f"       â†’ ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åãƒãƒƒãƒ")
                        if value_str in dialogue:
                            self.log_message(f"       â†’ ã‚»ãƒªãƒ•å†…å®¹ãƒãƒƒãƒ")
                
                # Analyze adjacency logic failure
                self.log_message(f"\\n   éš£æ¥ãƒ­ã‚¸ãƒƒã‚¯åˆ†æ:")
                for col_idx, (col_name, value) in enumerate(target_row.items()):
                    if pd.notna(value) and char_name in str(value):
                        self.log_message(f"     ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ç™ºè¦‹: åˆ—{col_idx}")
                        
                        # Check adjacent columns
                        for offset in [1, 2, 3, -1]:
                            adj_idx = col_idx + offset
                            if 0 <= adj_idx < len(target_row):
                                adj_col_name = list(df.columns)[adj_idx]
                                adj_value = target_row.iloc[adj_idx]
                                if pd.notna(adj_value):
                                    adj_str = str(adj_value).strip()
                                    self.log_message(f"       éš£æ¥åˆ—{adj_idx}(+{offset}): {adj_str[:80]}...")
                                    if adj_str in dialogue:
                                        self.log_message(f"         â†’ ã“ã‚ŒãŒèª¤ã£ã¦ã€Œã‚»ãƒªãƒ•ã€ã¨ã—ã¦æŠ½å‡ºã•ã‚ŒãŸ")
            
        except Exception as e:
            self.log_message(f"   âŒ æ§‹é€ åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    def analyze_extraction_patterns(self):
        """Analyze patterns in the extraction logic that cause misclassification"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            self.log_message("\\n" + "=" * 80)
            self.log_message("æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ")
            self.log_message("=" * 80)
            
            # Pattern 1: Instructions with bullet points
            cursor.execute('''
                SELECT COUNT(*) FROM character_dialogue_unified 
                WHERE LENGTH(character_name) > 0 AND dialogue_text LIKE '%ãƒ»%'
            ''')
            bullet_count = cursor.fetchone()[0]
            self.log_message(f"1. ç®‡æ¡æ›¸ãæŒ‡ç¤ºæ–‡ï¼ˆãƒ»å«ã‚€ï¼‰: {bullet_count}ä»¶")
            
            # Pattern 2: Instructions with parentheses
            cursor.execute('''
                SELECT COUNT(*) FROM character_dialogue_unified 
                WHERE LENGTH(character_name) > 0 AND (dialogue_text LIKE '%ï¼ˆ%' OR dialogue_text LIKE '%ï¼‰%')
            ''')
            paren_count = cursor.fetchone()[0]
            self.log_message(f"2. æ‹¬å¼§ä»˜ãæŒ‡ç¤ºæ–‡: {paren_count}ä»¶")
            
            # Pattern 3: Instructions with arrows
            cursor.execute('''
                SELECT COUNT(*) FROM character_dialogue_unified 
                WHERE LENGTH(character_name) > 0 AND dialogue_text LIKE '%â†’%'
            ''')
            arrow_count = cursor.fetchone()[0]
            self.log_message(f"3. çŸ¢å°ä»˜ãæŒ‡ç¤ºæ–‡: {arrow_count}ä»¶")
            
            # Pattern 4: Production instructions
            cursor.execute('''
                SELECT COUNT(*) FROM character_dialogue_unified 
                WHERE LENGTH(character_name) > 0 AND (
                    dialogue_text LIKE '%ä½œã‚Šæ–¹%' OR dialogue_text LIKE '%æ¼”å‡º%'
                    OR dialogue_text LIKE '%æ’®å½±%' OR dialogue_text LIKE '%ç·¨é›†%'
                )
            ''')
            production_count = cursor.fetchone()[0]
            self.log_message(f"4. åˆ¶ä½œæŒ‡ç¤ºæ–‡: {production_count}ä»¶")
            
            # Pattern 5: Character names as "characters"
            cursor.execute('''
                SELECT character_name, COUNT(*) as count
                FROM character_dialogue_unified 
                WHERE LENGTH(character_name) > 0 AND LENGTH(dialogue_text) > 0
                AND character_name NOT IN ('ã‚µãƒ³ã‚µãƒ³', 'ãã‚‚ã‚Šã‚“', 'ãƒ„ã‚¯ãƒ¢', 'ãƒã‚¤ã‚º', 'ãƒãƒ', 'ãƒ‘ãƒ‘', 'BB')
                GROUP BY character_name
                ORDER BY count DESC
                LIMIT 10
            ''')
            non_character_names = cursor.fetchall()
            
            self.log_message(f"\\n5. éã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åãŒã€Œã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã€ã¨ã—ã¦åˆ†é¡:")
            for name, count in non_character_names:
                self.log_message(f"   {name}: {count}ä»¶")
            
            # Analyze specific examples
            cursor.execute('''
                SELECT character_name, dialogue_text 
                FROM character_dialogue_unified 
                WHERE character_name = 'SE' AND LENGTH(dialogue_text) > 50
                LIMIT 5
            ''')
            se_examples = cursor.fetchall()
            
            self.log_message(f"\\n6. 'SE'ã¨ã—ã¦åˆ†é¡ã•ã‚ŒãŸä¾‹:")
            for char, dialogue in se_examples:
                self.log_message(f"   {char}: {dialogue[:100]}...")
            
            conn.close()
            
        except Exception as e:
            self.log_message(f"âŒ Pattern analysis error: {str(e)}")
    
    def analyze_adjacency_failures(self):
        """Analyze why adjacency logic fails"""
        self.log_message("\\n" + "=" * 80)
        self.log_message("éš£æ¥ãƒ­ã‚¸ãƒƒã‚¯å¤±æ•—åŸå› åˆ†æ")
        self.log_message("=" * 80)
        
        self.log_message("æ¨å®šã•ã‚Œã‚‹å¤±æ•—åŸå› :")
        self.log_message("1. è¤‡æ•°è¡Œã«ã¾ãŸãŒã‚‹ã‚»ãƒ«çµåˆ")
        self.log_message("   - ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åãŒè¤‡æ•°è¡Œåˆ†ã®ã‚»ãƒ«ã¨çµåˆã•ã‚Œã¦ã„ã‚‹")
        self.log_message("   - éš£æ¥ã™ã‚‹å…¨ã¦ã®è¡ŒãŒã€Œã‚»ãƒªãƒ•ã€ã¨ã—ã¦æŠ½å‡ºã•ã‚Œã‚‹")
        
        self.log_message("\\n2. ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆæ§‹é€ ã®ä¸çµ±ä¸€")
        self.log_message("   - ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åˆ—ã¨æŒ‡ç¤ºåˆ—ã®ä½ç½®ãŒä¸€å®šã§ãªã„")
        self.log_message("   - éš£æ¥ãƒ­ã‚¸ãƒƒã‚¯ãŒæŒ‡ç¤ºæ–‡ã®åˆ—ã‚’èª¤ã£ã¦ã‚»ãƒªãƒ•åˆ—ã¨åˆ¤å®š")
        
        self.log_message("\\n3. å†…å®¹ãƒ™ãƒ¼ã‚¹åˆ¤å®šã®ä¸å‚™")
        self.log_message("   - ã€Œãƒ»ã€ã€Œï¼ˆï¼‰ã€ã€Œâ†’ã€ç­‰ã®æŒ‡ç¤ºãƒãƒ¼ã‚«ãƒ¼ã‚’ç„¡è¦–")
        self.log_message("   - é•·æ–‡ã®åˆ¶ä½œæŒ‡ç¤ºã‚‚ã‚»ãƒªãƒ•ã¨ã—ã¦æŠ½å‡º")
        
        self.log_message("\\n4. ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åã®èª¤èªè­˜")
        self.log_message("   - 'SE'ã€'TRUE'ã€'FALSE'ç­‰ã‚’ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åã¨åˆ¤å®š")
        self.log_message("   - ã“ã‚Œã‚‰ã«éš£æ¥ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¨ã¦ã‚»ãƒªãƒ•ã¨ã—ã¦æŠ½å‡º")
        
        self.log_message("\\n5. æŠ½å‡ºæˆ¦ç•¥ã®éåº¦ãªè¨±å®¹æ€§")
        self.log_message("   - ã€Œã¨ã‚Šã‚ãˆãšä½•ã§ã‚‚æŠ½å‡ºã€ã®æ–¹é‡")
        self.log_message("   - æŒ‡ç¤ºæ–‡ã¨ã‚»ãƒªãƒ•ã®åŒºåˆ¥ã‚’ã—ãªã„æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯")
    
    def run_root_cause_analysis(self):
        """Run complete root cause analysis"""
        self.log_message("=" * 80)
        self.log_message("ã‚»ãƒªãƒ•ãƒ»æŒ‡ç¤ºæ–‡æ··åœ¨å•é¡Œã®æ ¹æœ¬åŸå› åˆ†æ")
        self.log_message("=" * 80)
        
        # 1. Analyze problematic entries
        problematic_entries = self.analyze_problematic_entries(10)
        
        # 2. Analyze extraction patterns
        self.analyze_extraction_patterns()
        
        # 3. Analyze adjacency logic failures
        self.analyze_adjacency_failures()
        
        # 4. Summary
        self.log_message("\\n" + "=" * 80)
        self.log_message("ğŸ“‹ æ ¹æœ¬åŸå› ã¾ã¨ã‚")
        self.log_message("=" * 80)
        self.log_message("âŒ ä¸»è¦å•é¡Œ:")
        self.log_message("1. éš£æ¥ãƒ­ã‚¸ãƒƒã‚¯ã ã‘ã§ã¯ä¸ååˆ†")
        self.log_message("   - ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®è¤‡é›‘ãªæ§‹é€ ã‚’è€ƒæ…®ã—ã¦ã„ãªã„")
        self.log_message("   - ã‚»ãƒ«çµåˆã€ä¸è¦å‰‡ãªé…ç½®ã«å¯¾å¿œã§ããªã„")
        
        self.log_message("\\n2. å†…å®¹ãƒ™ãƒ¼ã‚¹åˆ¤å®šã®æ¬ å¦‚")
        self.log_message("   - æŒ‡ç¤ºæ–‡ãƒãƒ¼ã‚«ãƒ¼ï¼ˆãƒ»ã€ï¼ˆï¼‰ã€â†’ï¼‰ã‚’ç„¡è¦–")
        self.log_message("   - ã‚»ãƒªãƒ•ã‚‰ã—ã•ã®åˆ¤å®šåŸºæº–ãŒç”˜ã„")
        
        self.log_message("\\n3. ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åèªè­˜ã®å•é¡Œ")
        self.log_message("   - SEã€TRUEã€FALSEç­‰ã‚’èª¤èªè­˜")
        self.log_message("   - å®Ÿåœ¨ã—ãªã„ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åã‚‚å—ã‘å…¥ã‚Œ")
        
        self.log_message("\\nâœ… å¿…è¦ãªå¯¾ç­–:")
        self.log_message("1. å³æ ¼ãªå†…å®¹ãƒ™ãƒ¼ã‚¹åˆ¤å®šã®å®Ÿè£…")
        self.log_message("2. æŒ‡ç¤ºæ–‡ãƒãƒ¼ã‚«ãƒ¼ã®é™¤å¤–ãƒ­ã‚¸ãƒƒã‚¯å¼·åŒ–")
        self.log_message("3. ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åã®å³æ ¼ãªæ¤œè¨¼")
        self.log_message("4. ã‚»ãƒªãƒ•ã‚‰ã—ã•ã®åˆ¤å®šåŸºæº–å¼·åŒ–")
        self.log_message("=" * 80)

def main():
    """Main execution function"""
    db_path = "/Users/mitsuruono/sunsun_script_search/sunsun_script_database/youtube_search_complete_all.db"
    
    analyzer = RootCauseAnalyzer(db_path)
    analyzer.run_root_cause_analysis()

if __name__ == "__main__":
    main()